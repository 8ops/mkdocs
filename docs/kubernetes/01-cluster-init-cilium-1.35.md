# Quick Start - Cilium - 1.35

[Reference](01-cluster-init.md)



> å½“å‰å„è½¯ä»¶ç‰ˆæœ¬

| åç§°       | ç‰ˆæœ¬   |
| ---------- | ------ |
| ubuntu     | 24.04  |
| kubernetes | 1.35.0 |
| flannel    | 0.27.3 |



## ä¸€ã€å‡†å¤‡èµ„æº

### 1.1 VIP

é€šè¿‡ `haproxy` ä»£ç† `apiserver` å¤šèŠ‚ç‚¹

**10.101.11.110**



### 1.2 æœåŠ¡å™¨

| role          | hostname       | ip            |
| ------------- | -------------- | ------------- |
| control-plane | K-KUBE-LAB-01  | 10.101.11.240 |
| control-plane | K-KUBE-LAB-02  | 10.101.11.114 |
| control-plane | K-KUBE-LAB-03  | 10.101.11.154 |
| worker-node   | K-KUBE-LAB-08  | 10.101.11.196 |
| worker-node   | K-KUBE-LAB-11  | 10.101.11.157 |
| worker-node   | K-KUBE-LAB-012 | 10.101.11.250 |



## äºŒã€æ­å»ºé›†ç¾¤



### 2.1 åˆå§‹åŒ–ç¯å¢ƒ

```bash
 curl -s https://books.8ops.top/attachment/kubernetes/bin/01-init-ubuntu24.04-v1.35.sh | bash
```

### 

```bash
 # æŸ¥çœ‹ç³»ç»Ÿéšå¼€æœºå¯åŠ¨æœåŠ¡
 systemctl list-unit-files --state=enabled
```



### 2.2 è°ƒæ•´libç›®å½•

```bash
# containerd
mkdir -p /data1/lib/containerd && \
    ([ -e /var/lib/containerd ] && mv /var/lib/containerd{,-$(date +%Y%m%d)} || /bin/true) && \
    ln -s /data1/lib/containerd /var/lib/containerd
ls -l /var/lib/containerd

# kubelet
mkdir -p /data1/lib/kubelet && \
    ([ -e /var/lib/kubelet ] && mv /var/lib/kubelet{,-$(date +%Y%m%d)} || /bin/true) && \
    ln -s /data1/lib/kubelet /var/lib/kubelet
ls -l /var/lib/kubelet   

# etcdï¼ˆä»…éœ€è¦åœ¨ control-planeï¼‰
mkdir -p /data1/lib/etcd && \
    ([ -e /var/lib/etcd ] && mv /var/lib/etcd{,-$(date +%Y%m%d)} || /bin/true) && \
    ln -s /data1/lib/etcd /var/lib/etcd
ls -l /var/lib/etcd
```



### 2.3 å®‰è£…å®¹å™¨è¿è¡Œæ—¶

```bash
CONTAINERD_VERSION=2.2.1-1~ubuntu.24.04~noble
apt install -y containerd.io=${CONTAINERD_VERSION}

apt-mark hold containerd.io
apt-mark showhold
dpkg -l | grep containerd.io

# ä½¿ç”¨ crictl æ›¿æ¢ ctr è¿è¡Œæ—¶
mkdir -p /etc/containerd
containerd config default > /etc/containerd/config.toml-default
cp /etc/containerd/config.toml-default /etc/containerd/config.toml

sed -i 's#registry.k8s.io/pause:3.10.1#hub.8ops.top/google_containers/pause:3.10.1#' /etc/containerd/config.toml
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml
sed -i 's#/etc/containerd/certs.d:/etc/docker/certs.d#/etc/containerd/certs.d#' /etc/containerd/config.toml
grep -P 'pause:|SystemdCgroup' /etc/containerd/config.toml

systemctl restart containerd && systemctl status containerd

# è°ƒæ•´æ—¥å¿—çº§åˆ«ï¼ˆdebugã€infoã€warnã€errorã€fatalã€panicï¼‰å’Œ prometheus ç›‘æ§æŒ‡æ ‡
# 27 [debug]
# 28   address = ''
# 29   uid = 0
# 30   gid = 0
# 31   level = 'warn' # å‡å°‘æ—¥å¿—å™ªå£°
# 32   format = ''
# 33
# 34 [metrics]
# 35   address = ':10254' # æš´éœ² Prometheus æŒ‡æ ‡
# 36   grpc_histogram = false
```



> å—ä¿¡ç§æœ‰CA

```bash
# 1
mkdir -p /etc/containerd/certs.d/hub.8ops.top
cp ca.crt /etc/containerd/certs.d/hub.8ops.top/ca.crt 

systemctl restart containerd && systemctl status containerd

crictl pull hub.8ops.top/google_containers/pause:3.10.1

# 2
mkdir -p /etc/containerd/certs.d/hub.8ops.top
cat > /etc/containerd/certs.d/hub.8ops.top/hosts.toml <<EOF
server = "https://hub.8ops.top"

[host."https://hub.8ops.top"]
  capabilities = ["pull", "resolve", "push"]
  skip_verify = true
EOF

# ä¸´æ—¶éªŒè¯
ctr -n k8s.io images pull \
  hub.8ops.top/google_containers/pause:3.10.1
ctr -n k8s.io images pull \
  --hosts-dir /etc/containerd/certs.d \
  hub.8ops.top/google_containers/pause:3.10.1
ctr -n k8s.io images rm hub.8ops.top/google_containers/pause:3.10.1
ctr -n k8s.io images ls

systemctl restart containerd && systemctl status containerd

crictl pull hub.8ops.top/google_containers/pause:3.10.1
crictl img
crictl rmi hub.8ops.top/google_containers/pause:3.10.1

# systemd-resolved ä¼šå¹²æ‰°è§£æ
systemctl stop systemd-resolved && systemctl disable systemd-resolved
sed -i -e '/^nameserver /i\nameserver 10.101.9.252' -e '/^nameserver 127.0.0.53/d' /etc/resolv.conf
cat /etc/resolv.conf && ping -c 2 hub.8ops.top
```



### 2.4 å®‰è£… kube ç¯å¢ƒ

```bash
# kubeadm
KUBERNETES_VERSION=1.35.0-1.1
apt install -y -q kubeadm=${KUBERNETES_VERSION} kubectl=${KUBERNETES_VERSION} kubelet=${KUBERNETES_VERSION}

apt-mark hold kubeadm kubectl kubelet
apt-mark showhold
dpkg -l | grep kube

# ç”¨äºè¿è¡Œ crictl
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF

systemctl restart containerd
crictl images
crictl ps -a

# åˆå§‹é›†ç¾¤ï¼ˆä»…éœ€è¦åœ¨å…¶ä¸­ä¸€å° control-plane èŠ‚ç‚¹æ“ä½œï¼‰
# config
export KUBE_VERSION=v1.35.0
mkdir -p /opt/kubernetes && cd /opt/kubernetes

kubeadm config print init-defaults > kubeadm-init.yaml-${KUBE_VERSION}-default
cp kubeadm-init.yaml-${KUBE_VERSION}-default kubeadm-init.yaml-${KUBE_VERSION}

kubeadm config images list
kubeadm config images list --config kubeadm-init.yaml-${KUBE_VERSION}
kubeadm config images pull --config kubeadm-init.yaml-${KUBE_VERSION}

# # origin
# registry.k8s.io/kube-apiserver:v1.35.0
# registry.k8s.io/kube-controller-manager:v1.35.0
# registry.k8s.io/kube-scheduler:v1.35.0
# registry.k8s.io/kube-proxy:v1.35.0
# registry.k8s.io/coredns/coredns:v1.13.1
# registry.k8s.io/pause:3.10.1
# registry.k8s.io/etcd:3.6.6-0
#
# # revision
# hub.8ops.top/google_containers/kube-apiserver:v1.35.0
# hub.8ops.top/google_containers/kube-controller-manager:v1.35.0
# hub.8ops.top/google_containers/kube-scheduler:v1.35.0
# hub.8ops.top/google_containers/kube-proxy:v1.35.0
# hub.8ops.top/google_containers/coredns:v1.13.1
# hub.8ops.top/google_containers/pause:3.10.1
# hub.8ops.top/google_containers/etcd:3.6.6-0

kubeadm init --config kubeadm-init.yaml-${KUBE_VERSION} --upload-certs

mkdir -p ~/.kube && ln -s /etc/kubernetes/admin.conf ~/.kube/config 

# æ·»åŠ èŠ‚ç‚¹ control-plane
kubeadm join 10.101.11.110:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:3acfc0056f88d86565bcf482358e62b7729b59192d2faf51f6f553731beb674b \
	--control-plane --certificate-key 7d7617c7635f4e2e32e16c6616bbe21007a6bafd131f7a2417f89be78a915174

# æ·»åŠ èŠ‚ç‚¹ work-node
kubeadm join 10.101.11.110:6443 --token abcdef.0123456789abcdef \
	--discovery-token-ca-cert-hash sha256:3acfc0056f88d86565bcf482358e62b7729b59192d2faf51f6f553731beb674b  
```

> ç¼–è¾‘ kubeadm-init.yaml-v1.35.0

```bash
apiVersion: kubeadm.k8s.io/v1beta4
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.101.11.240
  bindPort: 6443
nodeRegistration:
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  imagePullSerial: true
  name: K-KUBE-LAB-01
  taints: null
timeouts:
  controlPlaneComponentHealthCheck: 4m0s
  discovery: 5m0s
  etcdAPICall: 2m0s
  kubeletHealthCheck: 4m0s
  kubernetesAPICall: 1m0s
  tlsBootstrap: 5m0s
  upgradeManifests: 5m0s
---
apiServer: {}
apiVersion: kubeadm.k8s.io/v1beta4
caCertificateValidityPeriod: 87600h0m0s
certificateValidityPeriod: 8760h0m0s
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  imageRepository: hub.8ops.top/google_containers
  imageTag: v1.13.1
encryptionAlgorithm: RSA-2048
etcd:
  local:
    dataDir: /var/lib/etcd
    imageRepository: hub.8ops.top/google_containers
    imageTag: 3.6.6-0
imageRepository: hub.8ops.top/google_containers
kind: ClusterConfiguration
kubernetesVersion: 1.35.0
controlPlaneEndpoint: 10.101.11.110:6443
networking:
  dnsDomain: cluster.local
  podSubnet: 172.19.0.0/16
  serviceSubnet: 192.168.0.0/16
proxy: {}
scheduler: {}
```

### 2.5 ä¼˜åŒ–é…ç½®

*required*

#### 2.5.1 cgroup2

ä¸ºä»€ä¹ˆå¿…é¡»å‡çº§åˆ° cgroup v2

- v1.25+ **å®˜æ–¹é»˜è®¤æ¨è cgroup v2**

- systemd å·²ç»Ÿä¸€ä½¿ç”¨ v2

- CPU / Memory / IO è°ƒåº¦æ›´ç²¾å‡†

- eBPFã€Ciliumã€Sidecar æ€§èƒ½æ˜¾è‘—æå‡

```bash
# 1ï¼Œæ£€æµ‹æ˜¯å¦æ”¯æŒcgroup2ï¼ˆcgroupfs â†’ v1ã€cgroup2fs â†’ v2ï¼‰
stat -fc %T /sys/fs/cgroup

# 2ï¼ŒOS å¯ç”¨ cgroup v2
sed -i 's#GRUB_CMDLINE_LINUX=""#GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=1 cgroup_no_v1=all"#' /etc/default/grub
grep GRUB_CMDLINE_LINUX= /etc/default/grub
update-grub
reboot

# 3ï¼Œcontainerd é…ç½®
# /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
systemctl restart containerd

# 4ï¼Œkubelet å¯ç”¨ systemd cgroup driver
# /var/lib/kubelet/config.yaml
cgroupDriver: systemd

# 5ï¼ŒéªŒè¯
kubectl describe node | grep -i Cgroup
ls /sys/fs/cgroup/kubepods.slice/
```



#### 2.5.2 nftables

iptables â†’ nftables èƒŒæ™¯

- Ubuntu 22.04 é»˜è®¤ **iptables-nft**
- kube-proxy æ”¯æŒ nftables backend
- iptables-legacy ä¸ nft æ··ç”¨ = **ç¾éš¾**

```bash
# 1ï¼Œç»Ÿä¸€ç³»ç»Ÿé˜²ç«å¢™åç«¯
update-alternatives --set iptables /usr/sbin/iptables-nft
update-alternatives --set ip6tables /usr/sbin/ip6tables-nft
update-alternatives --set arptables /usr/sbin/arptables-nft
update-alternatives --set ebtables /usr/sbin/ebtables-nft

update-alternatives --list iptables

update-alternatives --display iptables

iptables -V
iptables v1.8.10 (nf_tables) # v1.8.0+ è‡ªåŠ¨è¯†åˆ« nf_tables

# 2ï¼Œkube-proxy nftables æ¨¡å¼
kubectl -n kube-system edit configmap kube-proxy
---
kubeletConfiguration:
  cgroupDriver: systemd
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "iptables"
iptables:
  backend: "nft"

# 3ï¼Œnftables åŸºç¡€æ”¾è¡Œè§„åˆ™ï¼ˆç¤ºä¾‹ï¼‰
table inet filter {
  chain input {
    type filter hook input priority 0;
    policy drop;

    iif lo accept
    ct state established,related accept

    tcp dport {22, 6443, 10250, 10257, 10259} accept
    udp dport {4789} accept   # CNI VXLANï¼ˆå¦‚ Flannelï¼‰
  }
}

# v1.35 ä½¿ç”¨ Ciliumï¼ˆeBPF æ¨¡å¼ï¼‰
# â†’ å‡ ä¹ä¸ä¾èµ– iptables/nftablesï¼Œæ€§èƒ½å’Œç¨³å®šæ€§æœ€å¥½ã€‚

kubectl apply -f kube-proxy.yaml
kubectl -n kube-system rollout restart ds kube-proxy

# 4ï¼Œåº”ç”¨åéªŒè¯
# 4.1 
kubectl -n kube-system logs ds/kube-proxy | grep nft
# Using iptables-nft backend

# 4.2 nftables è§„åˆ™æ˜¯å¦å­˜åœ¨ï¼ˆç¡®è®¤åœ¨ä½¿ç”¨nftablesçš„åˆ¤æ–­ä¾æ®ï¼‰
nft list ruleset | grep KUBE-SERVICES

# 4.3 ç¡®è®¤æœªæ··ç”¨ legacy
iptables-legacy -L

```

ä¸ CNI çš„å…³ç³»ï¼ˆé‡ç‚¹ï¼‰

| CNI        | nftables è¦æ±‚               |
| ---------- | --------------------------- |
| Flannel    | VXLAN UDP 4789              |
| Calico     | BGP TCP 179                 |
| Cilium     | **æ— éœ€ kube-proxyï¼ˆeBPFï¼‰** |
| MetalLB L2 | ARP / NDP æ”¾è¡Œ              |



kube-proxy ConfigMap å…³é”®é…ç½®

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
  labels:
    app: kube-proxy
data:
  config.conf: |-
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration

    mode: iptables

    bindAddress: 0.0.0.0
    bindAddressHardFail: false

    clusterCIDR: 172.19.0.0/16

    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 1s
      syncPeriod: 30s              # âœ… é™ä½ nft churn

    conntrack:
      maxPerCore: 32768            # âœ… é€‚åˆ 6 èŠ‚ç‚¹
      min: 131072
      tcpEstablishedTimeout: 24h
      tcpCloseWaitTimeout: 1h

    configSyncPeriod: 30s          # âœ… ä» 5s â†’ 30s

    clientConnection:
      kubeconfig: /var/lib/kube-proxy/kubeconfig.conf
      qps: 0
      burst: 0

    metricsBindAddress: 127.0.0.1:10249
    healthzBindAddress: 0.0.0.0:10256

```



*optional*

```bash
# kubelet
kubectl -n kube-system get  configmap kubelet-config -o yaml > configmap-kubelet-config.yaml
kubectl -n kube-system edit configmap kubelet-config
â€¦â€¦
    # GC
    imageGCLowThresholdPercent: 40
    imageGCHighThresholdPercent: 50
    # Resource
    systemReserved:
      cpu: 500m
      memory: 500m
    kubeReserved:
      cpu: 500m
      memory: 500m
    evictionPressureTransitionPeriod: 300s # upgrade
    nodeStatusReportFrequency: 10s         # upgrade
    nodeStatusUpdateFrequency: 10s         # upgrade
    cgroupDriver: systemd
    maxPods: 200
    resolvConf: /etc/resolv.conf
kind: ConfigMap                            # relative
â€¦â€¦

# kube-proxy å‡çº§ä¸º iptables -> nftables
kubectl -n kube-system get  configmap kube-proxy -o yaml > configmap-kube-proxy.yaml
kubectl -n kube-system edit configmap kube-proxy

```



### 2.6 ç»­ç­¾ç»„ä»¶è¯ä¹¦

[Reference](06-cluster-renew-certs.md)

```bash
kubeadm certs check-expiration

# backup
cp -r /etc/kubernetes/pki{,-$(date +%Y%m%d)}
cp -r /etc/kubernetes/manifests{,-$(date +%Y%m%d)}
mv /usr/bin/kubeadm{,-$(kubeadm version -o short)}

# upgrade binary
curl -k -s -o /usr/bin/kubeadm https://filestorage.8ops.top/ops/kube/kubeadm-v1.35.0.amd64-10y
chmod +x /usr/bin/kubeadm

# renew
kubeadm certs renew all
cd /etc/kubernetes
mv manifests manifests-b && sleep 60 && mv manifests-b manifests
systemctl restart kubelet

# check
kubeadm certs check-expiration

# release
crictl ps -a | awk '/Exited/{printf("crictl rm %s\n",$1)}' | sh
```





## ä¸‰ã€åº”ç”¨ Cilium

```bash
CILIUM_VERSION=1.18.5
helm repo add cilium https://helm.cilium.io/
helm repo update cilium
helm search repo cilium
helm show values cilium/cilium \
  --version ${CILIUM_VERSION} > cilium.yaml-${CILIUM_VERSION}-default

helm install cilium cilium/cilium \
  -f cilium.yaml-${CILIUM_VERSION} \
  --namespace=kube-system \
  --version ${CILIUM_VERSION}

helm upgrade --install cilium cilium/cilium \
  -f cilium.yaml-${CILIUM_VERSION} \
  --namespace=kube-system \
  --version ${CILIUM_VERSION}
```



## å››ã€Addon

### 4.1 MetalLB

```bash
METALLB_VERSION=0.15.3

helm repo add metallb https://metallb.github.io/metallb
helm repo update metallb
helm search repo metallb
helm show values metallb/metallb \
  --version ${METALLB_VERSION} > metallb.yaml-${METALLB_VERSION}-default

helm install metallb metallb/metallb \
  -f metallb.yaml-${METALLB_VERSION} \
  --namespace=kube-server \
  --create-namespace \
  --version ${METALLB_VERSION}

kubectl apply -f 10-metallb-ipaddresspool.yaml
kubectl apply -f 10-metallb-l2advertisement.yaml
```



### 4.2 Ingress-Nginx

```bash
INGRESS_NGINX=4.14.1
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update ingress-nginx
helm search repo ingress-nginx
helm show values ingress-nginx/ingress-nginx \
  --version ${INGRESS_NGINX} > ingress-nginx.yaml-${INGRESS_NGINX}-default

helm install ingress-nginx-external-controller ingress-nginx/ingress-nginx \
  -f ingress-nginx.yaml-${INGRESS_NGINX} \
  -n kube-server \
  --create-namespace \
  --version ${INGRESS_NGINX}

kubectl -n kube-server create secret tls tls-8ops.top \
  --cert=8ops.top.crt \
   --key=8ops.top.key \
  --dry-run=client -o yaml > tls-8ops.top.yaml

kubectl -n kube-server exec -it ingress-nginx-external-controller-external-77947d57f4-4mspp -c controller -- bash

```

> åˆ‡å‰²æ—¥å¿—

```bash
# ubuntu 24.04 uid=101 is messagebus
# /etc/logrotate.d/nginx
/var/log/nginx/access.log
/data1/log/nginx/*/access.log
 {
    su messagebus nginx-ingress
    hourly
    rotate 180
    dateext
    missingok
    notifempty
    compress
    delaycompress
    nomail
    sharedscripts
    postrotate
        for pid in `/bin/pidof nginx `;do
            kill -USR1 ${pid}
        done
    endscript
}
/var/log/nginx/error.log
/data1/log/nginx/*/error.log
 {
    su messagebus nginx-ingress
    daily
    rotate 7
    dateext
    missingok
    notifempty
    compress
    delaycompress
    nomail
    sharedscripts
    postrotate
        for pid in `/bin/pidof nginx `;do
            kill -USR1 ${pid}
        done
    endscript
}

# ç¡®ä¿uid=101,gid=82çš„ç”¨æˆ·å’Œç»„å­˜åœ¨
groupadd -g 82 nginx-ingress
mkdir -p /data1/log/nginx && cd /data1/log/nginx
chown 101:82 * && ls -l 

systemctl start logrotate && ls -l && sleep 5 && systemctl status logrotate

# è°ƒæ•´å®šæ—¶å™¨ä¸ºå°æ—¶
command -v logrotate || apt install -y -q logrotate
sed -i 's/OnCalendar=daily/OnCalendar=hourly/' /lib/systemd/system/logrotate.timer
systemctl daemon-reload && sleep 5 && systemctl status logrotate.timer

tree /data1/log/nginx
```



### 4.3 Dashboard

```bash
KUBERNETES_DASHBOARD_VERSION=7.14.0
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm repo update kubernetes-dashboard
helm search repo kubernetes-dashboard
helm show values kubernetes-dashboard/kubernetes-dashboard \
  --version ${KUBERNETES_DASHBOARD_VERSION}> kubernetes-dashboard.yaml-${KUBERNETES_DASHBOARD_VERSION}-default

helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
    -f kubernetes-dashboard.yaml-${KUBERNETES_DASHBOARD_VERSION} \
    -n kube-server \
    --create-namespace \
    --version ${KUBERNETES_DASHBOARD_VERSION}

#----
# create sa for guest
kubectl create serviceaccount dashboard-guest -n kube-server

# binding clusterrole
kubectl create clusterrolebinding dashboard-guest \
  --clusterrole=view \
  --serviceaccount=kube-server:dashboard-guest

# create token
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-guest-secret
  namespace: kube-server
  annotations:
    kubernetes.io/service-account.name: dashboard-guest
type: kubernetes.io/service-account-token
EOF

# output token
kubectl -n kube-server get secrets dashboard-guest-secret -o=jsonpath={.data.token} | base64 -d && echo

#----
# create sa for ops
kubectl create serviceaccount dashboard-ops -n kube-server

# binding clusterrole
kubectl create clusterrolebinding dashboard-ops \
  --clusterrole=cluster-admin \
  --serviceaccount=kube-server:dashboard-ops

# create token
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-ops-secret
  namespace: kube-server
  annotations:
    kubernetes.io/service-account.name: dashboard-ops
type: kubernetes.io/service-account-token
EOF

# output token
kubectl -n kube-server get secrets dashboard-ops-secret -o=jsonpath={.data.token} | base64 -d && echo
```



## äº”ã€å¼‚å¸¸

### 5.1 QXL

æŠ¥é”™

```bash
Dec 31 13:44:12 K-KUBE-LAB-11 kernel: [TTM] Buffer eviction failed Dec 31 13:44:12 K-KUBE-LAB-11 kernel: qxl 0000:00:02.0: object_init failed for (4096, 0x00000001) Dec 31 13:44:12 K-KUBE-LAB-11 kernel: [drm:qxl_alloc_bo_reserved [qxl]] *ERROR* failed to allocate VRAM BO
```

åŸå› 

```bash
è¿™æ˜¯ Linux DRM å­ç³»ç»Ÿ + QXL æ˜¾å¡é©±åŠ¨ æŠ¥é”™ï¼š
qxlï¼šğŸ‘‰ QEMU / KVM / SPICE è™šæ‹Ÿæ˜¾å¡
TTMï¼šğŸ‘‰ æ˜¾å­˜å†…å­˜ç®¡ç†æ¨¡å—
failed to allocate VRAM BOï¼šğŸ‘‰ è™šæ‹Ÿæ˜¾å­˜ä¸è¶³æˆ–ä¸è¯¥è¢«ä½¿ç”¨

å…¸å‹å‡ºç°ç¯å¢ƒ
âœ… å‡ ä¹ 100% å‡ºç°åœ¨ï¼š
KVM / Proxmox / OpenStack
è™šæ‹Ÿæœº æ²¡æœ‰å›¾å½¢ç•Œé¢
ä½†ä»åŠ è½½äº† qxl / drm é©±åŠ¨

ä¸kubernetesæ— å…³
```

ä¿®å¤

```bash
# ç¦ç”¨ qxl é©±åŠ¨ï¼Œé™åœ¨kvmä¸­ä½¿ç”¨
cat <<EOF >/etc/modprobe.d/blacklist-qxl.conf
blacklist qxl
blacklist drm_kms_helper
EOF
update-initramfs -u
reboot
```

### 5.2 kernel: workqueue

æŠ¥é”™

```bash
Dec 31 15:23:46 K-KUBE-LAB-01 kernel: workqueue: drm_fb_helper_damage_work hogged CPU for >10000us 32 times, consider switching to WQ_UNBOUND
```

åŸå› 

é€é¡¹å«ä¹‰

| å­—æ®µ                        | å«ä¹‰                          |
| --------------------------- | ----------------------------- |
| `workqueue`                 | å†…æ ¸å·¥ä½œé˜Ÿåˆ—                  |
| `drm_fb_helper_damage_work` | DRM framebuffer åˆ·æ–°ä»»åŠ¡      |
| `hogged CPU`                | å ç”¨ CPU æ—¶é—´è¿‡é•¿             |
| `>10000us`                  | å•æ¬¡ >10ms                    |
| `32 times`                  | è¿ç»­è§¦å‘                      |
| `WQ_UNBOUND`                | å»ºè®®æ¢æˆéç»‘å®š CPU çš„å·¥ä½œé˜Ÿåˆ— |

```bash
å†…æ ¸æ­£åœ¨å°è¯•åˆ·æ–°ä¸€ä¸ªâ€œæ ¹æœ¬æ²¡äººç”¨çš„è™šæ‹Ÿæ˜¾å¡ framebufferâ€ï¼Œ
ç»“æœè¿™ä¸ªä»»åŠ¡åœ¨ CPU ä¸Šåå¤ç©ºè½¬ï¼Œå ç”¨æ—¶é—´è¿‡é•¿ï¼Œäºæ˜¯å†…æ ¸å‘å‡ºè­¦å‘Šã€‚

```

è§£å†³

```bash
# é™åœ¨kvmä¸­ä½¿ç”¨
cat <<EOF >/etc/modprobe.d/blacklist-drm.conf
blacklist qxl
blacklist virtio_gpu
blacklist drm
blacklist drm_kms_helper
blacklist fbdev
blacklist vesafb
blacklist efifb
EOF
update-initramfs -u
reboot
```





### 5.3  Cilium / MetalLBæ—¥å¿—æŠ¥é”™

æŠ¥é”™

```bash
Dec 31 13:43:49 K-KUBE-LAB-11 kubelet[840]: E1231 13:43:49.503389 840 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-system/cilium-operator-54bfddc4b-cjvcx" containerName="cilium-operator" Dec 31 13:43:55 K-KUBE-LAB-11 kubelet[840]: E1231 13:43:55.503070 840 prober_manager.go:209] "Readiness probe already exists for container" pod="kube-server/metallb-speaker-xfb6v" containerName="speaker"
```

åŸå› 

```bash
# è¿™æ˜¯ kubelet çš„ä¸€ä¸ªå·²çŸ¥è¡Œä¸ºæ—¥å¿—ï¼Œå«ä¹‰æ˜¯ï¼š
kubelet åœ¨ pod é‡å»º / çŠ¶æ€å›æ”¶æ—¶
å°è¯•é‡å¤æ³¨å†Œ readiness probe
å‘ç° probe å·²å­˜åœ¨ â†’ æ‰“ä¸€æ¡ Error æ—¥å¿—

# âš ï¸ æ³¨æ„
ä¸æ˜¯ Pod é”™è¯¯
ä¸æ˜¯ Readiness å¤±è´¥
ä¸æ˜¯ Probe å†²çª
åªæ˜¯ kubelet å†…éƒ¨çŠ¶æ€æœºæ—¥å¿—

# ä¸ºä»€ä¹ˆé›†ä¸­å‡ºç°åœ¨ Cilium / MetalLBï¼Ÿ
åŸå› éå¸¸æ¸…æ¥šï¼š
ğŸ”¹ Cilium Operator
leader election
operator pod ä¼šé¢‘ç¹ reconcile
readiness probe ç”Ÿå‘½å‘¨æœŸå¤æ‚
ğŸ”¹ MetalLB Speaker
DaemonSet
ä¸ node ç½‘ç»œäº‹ä»¶å¼ºç»‘å®š
Node çŠ¶æ€å˜åŒ–æ—¶ probe é‡å»ºæ¦‚ç‡é«˜

# æ˜¯å¦å½±å“æœåŠ¡å¯ç”¨æ€§ï¼Ÿ
å®Œå…¨ä¸å½±å“
```

ä¿®å¤

```bash
# å¿½ç•¥
# OR
# é™ä½ kubelet è¾“å‡ºæ—¥å¿—çº§åˆ«ç”±--v=4åˆ°--v=2ï¼ˆä¸å»ºè®®ï¼‰
```

